---
title: "Statistical Tests and Models"
output: html_document
date: "2024-06-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(palmerpenguins)
library(tidyverse)
library(broom)

pgns <- penguins %>% 
  drop_na() %>%
  mutate(
    species = factor(species),
    island = factor(island),
    sex = factor(sex),
    year = factor(year)
  )
```

Although I won't dive too deeply into the theoretical aspects of it, I will still present the tests in a formal definition. I will also provide a brief explanation of the test and how to interpret the results.

# Introduction

For some of these illustrations I will use simulated data. The functions we commonly use for simulation are `sample()` for randomly choosing a sample from data, and `rnorm()`, `rbinom()`, `runif()` (and more depending on the type of distribution you are interested in!) for sampling random variables from a distribution.

Another very useful function for simulation is `replicate()` function; it is a wrapper for repeatedly running a piece of code, usually involving random number generation.

To illustrate `sample()` function, imagine that rolling a fair 6-sided die 5 times. To simulate this, I could write:

```{r}
sample(
  x = 1:6, # defines a set of possible outcomes from a random experiment
  size = 5, # number of times I would like to sample
  replace = TRUE # TRUE indicates that I could get the same sample in the sampling procedure
)
```

In the long run, the proportion of each outcome should be roughly equal to 1/6.

```{r}
x <- sample(x = 1:6, size = 1e6, replace = TRUE)
x <- factor(x, 1:6)

prop.table(table(x)) %>%
  as.data.frame() %>%
  ggplot() +
  geom_col(aes(x = x, y = Freq)) +
  geom_hline(yintercept = 1/6, color = "red", linewidth = 1.5, linetype = "dashed") +
  ggtitle("Proportion of Each Outcome", "Red Line at y = 1/6")
```

It does look close enough, so the die must be fair. Instead of looking at the plot, we could also quantitatively determine if we should believe that the die is fair by using statistical tests.

## Quick Note on Hypothesis Testing

Hypothesis testing is a procedure in which you make a statement about a population and you put that to the test. Typically you define a null hypothesis ($H_0$) and alternative hypothesis ($H_a$ or $H_1$) that depends on your research question. The null hypothesis will dictate how your data **should** be distributed. You then use the data obtained to measure the evidence **against** the null hypothesis.

One of the ways we quantify this is with p-value, which measures the probability of observing the data, or data with more unlikely outcome under the null hypothesis (assuming that the model and the sample is representative of the population).

As I present the statistical models and tests, I will also include the null and alternative hypotheses to illustrate what is being tested in these procedures.

## Word About assumptions

## Categorical Data Analysis

### Chi-Squared Test for Goodness-of-Fit

First test we will look at is a Chi-Squared Goodness-of-Fit test. This tests if a categorical outcome follows a certain distribution.

Assumptions:

-   The variable of interest is categorical
-   Levels in the categorical variables are mutually exclusive
-   The observations are independent of one another
-   Expected value of cells should be \> 5

$H_0: p_1 = p_2 = p_3 = p_4 = p_5 = p_6 = \frac{1}{6}$

$H_0: \text{At least one of } p_i \neq \frac{1}{6}$

```{r}
x_table <- table(x)
chisq.test(x = x_table, p = rep(1/6, 6))
```

### Chi-Squared Test of Independence

Assumptions:

-   The two variables are categorical
-   Levels in the categorical variable are mutually exclusive
-   The observations are independent of one another
-   Expected value of cells should be \> 5

For this exercise we will use a `UCBAdmissions` dataset, which is available at `data/UCBAdmissions.csv`

```{r}
ucbdf <- read.csv("data/UCBAdmission2.csv")
ucbdf <- ucbdf %>%
  mutate(Admit = factor(Admit), Sex = factor(Sex))
head(ucbdf)
```

Each row represents a student with information about the student's sex, department the student applied to, and the final admit decision.

Let's see if there is relationship between admit decision and sex. Since they are both categorical variables, we can use Chi-Squared test of Independence.

$H_0: \text{Admit decision and sex are independent}$

$H_a: \text{Admit decision and sex are not independent}$

```{r}
chisq.test(x = ucbdf$Admit, y = ucbdf$Sex)
```

Looks like we get a significant result. Let's visualize the data to see the differences.

```{r}
ucbdf %>%
  group_by(Sex, Admit) %>%
  count() %>%
  group_by(Sex) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot() +
  geom_col(aes(x = Sex, y = prop, fill = Admit))
```

### Exercise 1

Chi squared test for each subset?

```{r}
ucbdf %>%
  group_by(Dept, Sex, Admit) %>%
  count() %>%
  group_by(Dept, Sex) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot() +
  facet_wrap(~Dept) +
  geom_col(aes(x = Sex, y = prop, fill = Admit))
```

## Continuous Variable (Mean Comparison)

### One Sample T-Test

One sample t-test can be used on a continuous variable to test if the population mean is different from a specific value.

Assumptions:

-   The variable is continuous
-   The observations are independent of one another
-   The data is approximately normally distributed
-   The dependent variable does not contain (spurious) extreme outliers

Suppose I am interested in testing if the mean body mass of `Adelie` penguins is 3500.

$H_0: \mu_{mass,Adelie} = 3500$

$H_a: \mu_{mass,Adelie} \neq 3500$

```{r}
pgns_adelie <- pgns %>%
  filter(species == "Adelie")

ggplot(pgns_adelie) +
  geom_boxplot(aes(y = body_mass_g))

ggplot(pgns_adelie) +
  geom_histogram(aes(x = body_mass_g), binwidth = 50)

t.test(pgns_adelie$body_mass_g)
```

### Two Sample T-Test

Two sample t-test is used to compare if the population means of two groups are equal or not.

Assumptions:

-   The variable is continuous
-   The observations are independent of one another
-   The data in each group is approximately normally distributed
-   The dependent variable does not contain (spurious) extreme outliers

Suppose I am interested in testing if the mean body mass of `Adelie` vs `Gentoo` penguins.

$H_0: \mu_{mass,Adelie} = \mu_{mass,Gentoo}$

$H_a: \mu_{mass,Adelie} \neq \mu_{mass,Gentoo}$

```{r}
pgns %>%
  filter(species %in% c("Adelie", "Gentoo")) %>%
  ggplot(aes(x = species, y = body_mass_g, fill = species)) +
  geom_boxplot(show.legend = FALSE) +
  stat_summary(geom = "point", fun = mean, show.legend = FALSE, pch = "x", size = 5)

pgns %>%
  filter(species %in% c("Adelie", "Gentoo")) %>%
  ggplot(aes(x = species, y = body_mass_g, fill = species)) +
  geom_violin(show.legend = FALSE) +
  stat_summary(geom = "point", fun = mean, show.legend = FALSE, pch = "x", size = 5)
```

```{r}
t.test(body_mass_g ~ species, data = pgns %>% filter(species %in% c("Adelie", "Gentoo")))
```

The first argument passed into the `t.test()` is a formula object, which we will go over soon. What you should know right now is that the variable on the left is the dependent variable and the variable on the right is the independent variable. In the example above, we are comparison the mean of body mass (dependent variable), which depends on the species of the penguin (independent variable).

### One-way ANOVA

If you have more than two groups in a categorical independent variables and you would like to test if the groups means are all equal, you can use ANOVA.

ANOVA (Analysis of Variance) is a type of model for comparison of means between three or more groups. There are different flavors of ANOVA, each of which are suitable for different experimental conditions. If we are comparing the means of a continuous variable across groups from one independent variable, we can use **one-way ANOVA**.

This is an omnibus test statistic, meaning that if any pair of means is different, then the test will show statistical significance.

Assumptions:

-   The dependent variable is continuous
-   The observations are independent of one another
-   The data in each group is approximately normally distributed
-   Variance of the data in the different groups should be approximately equal\*
-   The dependent variable does not contain (spurious) extreme outliers

Instead of testing the mean of body mass between two species, we can extend the analysis to see if the means across all groups are equal.

$H_0: \mu_{mass,Adelie} = \mu_{mass,Gentoo} = \mu_{mass,Chinstrap}$

$H_a: \text{At least one } \mu_i \text{ not equal}$

```{r}
pgns %>%
  ggplot(aes(x = species, y = body_mass_g, fill = species)) +
  geom_boxplot(show.legend = FALSE) +
  stat_summary(geom = "point", fun = mean, show.legend = FALSE, pch = "x", size = 5)
```

```{r}
my_aov <- aov(body_mass_g ~ species, data = pgns)
print(my_aov)
```

### Exercise

Compare `body_mass_g` across islands. Plot distributions to check assumptions, and then run `aov()`.

### R Formula Object

There's one more syntax we should learn in R, which is a `formula` object. If you have done some form of modeling in R, you may have encountered them already.

This is a what a formula object looks like:

```{r}
# a two-sided formula
lhs ~ rhs
```

Presence of `~` tells R that this is going to be a formula object. The meaning of the formula depends on what the function using the formula wants to do this this. Usually in the context of modeling, the formula represents a relationship between the dependent variable (left hand side; `lhs`) and the independent variable(s) (right hand side; `rhs`).

Most often you will use this syntax to specify the relationship between dependent variable and independent variables in your statistical models such as linear regression (`lm()`) and generalized linear model (`glm()`).

This isn't always a case though; a formula object can also be one-sided, with the left hand side being empty. For example, `ggplot2`'s `facet_wrap()` function accepts a one sided formula object to define the stratification variable for faceting

```{r}
ggplot(pgns) +
  facet_wrap(~species) + # one-sided formula to stratify by species
  geom_point(aes(x = flipper_length_mm, y = body_mass_g))
```

There are special operators that you can use in the formula object to indicate special relationships. For the remainder of this workshop, we will be working with linear models. So let's look at different operators and translate them to the linear model.

-   `+`: add variables

```{r}
y ~ x1 + x2 + x3
```

$y = \beta_0 + \beta1x_1 + \beta_2x_2 + \beta3x_3$

-   `-`: remove variables. This is commonly seen being used when removing an intercept.

```{r}
y ~ -1 + x1 + x2
```

$y = \beta1x_1 + \beta_2x_2$

-   `*`: levels plus interaction

```{r}
y ~ x1*x2
```

$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2$

-   `:`: interaction

```{r}
y ~ x1:x2
```

$y = \beta_0 + \beta_1x_1x_2$

NOTE: `y ~ x1*x2` is equivalent to `y ~ x1 + x2 + x1:x2`

-   `^`: This one is tricky. This alone doesn't mean raise a variable to the power or something. It indicates higher order interaction

```{r}
y ~ (x1 + x2 + x3)^2
```

$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + \beta_4x_1x_2 + \beta_5x_1x_3 + \beta_6x_2x_3$

-   `I()`: "As-**I**s". You should use this for polynomial regression.

```{r}
y ~ x1 + I(x2^2)
```

$y = \beta_0 + \beta_1x_1 + \beta_2{x_2}^2$

-   Alternatively, you can use `poly()` for polynomial regression, which automatically includes lower order terms for the variable.

```{r}
y ~ x1 + poly(x2, degree = 2)
```

$y = \beta_0 + \beta_1x_1 + \beta_2x_2 \beta_3{x_2}^2$

## Linear Regression

Linear regression is used to model a linear relationship between a dependent variable and multiple predictors. For simplicity, I will call the dependent variable Y and predictors X.

The mathematical formula for linear regression can be represented as

$Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + ... + \beta_pX_{ip} + \epsilon_i$

Where $Y_i$ is the outcome of the ith observation, $X_ik$ is the ith observation of the kth dependent variable, and $\epsilon_i$ is the error term

Assumptions: - There is a linear relationship between X and the mean of Y - Observations are independent of each other. - The variance of the error term is the same for any value of X (homoskedasticity) - The error term is normally distributed with mean 0

First let's try simple linear regression. Suppose I am modelling a relationship between flipper length and body mass.

```{r}
ggplot(pgns) + 
  geom_point(aes(x = flipper_length_mm, y = body_mass_g)) +
  ggtitle("Body Mass (g) vs Flipper Length (mm)")
```

For linear regression, you can use `lm()` function. Use `summary()` on the model output to get the summary of the result.

```{r}
mymodel <- lm(body_mass_g ~ flipper_length_mm, data = pgns)
summary(mymodel)
```

One way to check model assumptions is to directly use `plot()` function on the model output.

```{r}
plot(mymodel)
```

With the first plot (Residuals vs Fitted), you can check if the linearity assumption and constant variance assumption is being met.

The second plot (Q-Q Residuals) checks if your error term (residual) is normally distributed. You would like the points to roughly lie on the diagonal line.

The third plot also helps you diagnose non constant variance.

The last plot is a little more complicated, but in this plot you look for points with high leverage (x-axis) with large absolute residual value (y-axis). These are points that can influence the final output of your linear model.

Extending the model to multiple parameters is also straightforward. Let's try a more complicated model. I'll model body mass against species, island, sex, bill length, bill depth, and flipper length.

```{r}
mymodel2 <- lm(body_mass_g ~ species + island + sex + bill_length_mm + bill_depth_mm + flipper_length_mm, data = pgns)
summary(mymodel2)
```

Here are the plots for model diagnostics

```{r}
plot(mymodel2)
```

Exercise:

In previous ANOVA example we saw significant differences between islands, but in linear regression it now looks like the effects are not statistical significant. Can you make a plot to explain why this is a case?

```{r}
ggplot(pgns) +
  geom_boxplot(aes(x = island, y = body_mass_g, fill = species))
```

Exercise:

After looking at this plot, I also suspect that there is an interaction between species and sex. In other words, I suspect that the relationship between sex and body weight depends on the species. Can you create a linear model to capturer this behavior? What p-value did you get?

```{r}
ggplot(pgns) + 
  geom_boxplot(aes(x = species, y = body_mass_g, fill = sex))
```

```{r}
mymodel3 <- lm(body_mass_g ~ species * sex + island + bill_length_mm + bill_depth_mm + flipper_length_mm, data = pgns)
summary(mymodel3)
plot(mymodel3)
```

## Using `broom()` Package

Instead of just looking at the model output, you might also want to retrieve some aspects of the model as a data frame for visualization or comparison across models.

This is how it would look like in base R:

-   Getting coefficient, standard error, and p-value.

```{r}
summary(mymodel2)$coefficients
```

-   Getting residuals

```{r}
head(residuals(mymodel2))
# or
head(mymodel2$residuals)
```

-   Getting fitted values from the original data

```{r}
head(mymodel2$fitted.values)
```

There are several problems with this. First of all, that's a lot of syntax to memorize, and there's no guarantee that the syntax / names of these components are the same across different model objects. Also, you might refer back to the original data when looking at residuals and fitted values, so it would be nice to have a way to retrieve all of them together. This is where the `broom` package shines: it takes the messy output of models and transform them into tidy table format.

There are 3 core functions in this package:

-   `tidy`: construct a table that summarizes model's statistical findings.
-   `augment`: add columns to the original data set that was modeled (e.g. fitted value, residual).
-   `glance`: construct a one-row summary of the entire model.

The `broom` tidies models from popular modelling packages and supports most model objects that come with base R. You can check out which methdos are supported with [`vignette("available-methods")`](https://broom.tidymodels.org/articles/available-methods.html).

```{r}
tidy(mymodel2)
augment(mymodel2)
```

New columns start with `.` to avoid overwriting original columns.

```{r}
glance(mymodel2)
```

`broom` allows me to explore the model more easily; for example, I can recreate the diagnostic plot, and even add more information.

```{r}
mymodel2 %>%
  augment() %>%
  mutate(rn = row_number()) %>%
  ggplot(aes(x = .hat, y = .std.resid, color = sex)) +
  geom_point() +
  geom_text(aes(label = rn), vjust = 1.5) +
  ggtitle("Recreate Residuals vs Leverage")
```
